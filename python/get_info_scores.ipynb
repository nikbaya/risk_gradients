{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_set = 'hm3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Spark and Hail with default parameters...\n",
      "Running on Apache Spark version 2.2.3\n",
      "SparkUI available at http://10.128.0.106:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.12-13681278eb89\n",
      "LOGGING: writing to /home/hail/hail-20190611-1201-0.2.12-13681278eb89.log\n"
     ]
    }
   ],
   "source": [
    "mt = hl.read_matrix_table(f'gs://nbaya/split/ukb31063.{variant_set}_variants.gwas_samples_repart.mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-11 15:43:43 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:43:50 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:43:50 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:43:55 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:43:55 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:00 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:01 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:05 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:05 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:09 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:10 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:14 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:14 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:17 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:18 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:21 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:22 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:25 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:25 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:29 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:29 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:32 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:32 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:35 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:36 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:38 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:38 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:41 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:41 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:43 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:43 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:46 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:46 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:48 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-11 15:44:49 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:51 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:51 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:53 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:53 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:54 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:55 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:56 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n",
      "2019-06-11 15:44:56 Hail: INFO: Reading table to impute column types\n",
      "2019-06-11 15:44:57 Hail: INFO: Finished type imputation\n",
      "  Loading column 'f0' as type 'str' (imputed)\n",
      "  Loading column 'f1' as type 'str' (imputed)\n",
      "  Loading column 'f2' as type 'int32' (imputed)\n",
      "  Loading column 'f3' as type 'str' (imputed)\n",
      "  Loading column 'f4' as type 'str' (imputed)\n",
      "  Loading column 'f5' as type 'float64' (imputed)\n",
      "  Loading column 'f6' as type 'str' (imputed)\n",
      "  Loading column 'f7' as type 'float64' (imputed)\n"
     ]
    }
   ],
   "source": [
    "gt0 = hl.read_matrix_table('gs://phenotype_31063/hail/imputed/ukb31063.GT.autosomes.mt/')\n",
    "tb_ls = [hl.import_table(f'gs://fc-7d5088b4-7673-45b5-95c2-17ae00a04183/imputed/ukb_mfi_chr{i}_v3.txt',impute=True,no_header=True).key_by('f0') for i in range(1,23)]\n",
    "tb = tb_ls[0]\n",
    "tb = tb.union(*[tb_ls[i] for i in range(1,22)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tb_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-11 15:48:24 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 15:48:30 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-78f3927a3065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://nbaya/risk_gradients/info_scores_all.ht'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-932>\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, output, overwrite, stage_locally, _codec_spec)\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/table.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, output, overwrite, stage_locally, _codec_spec)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \"\"\"\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTableWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTableNativeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_locally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_codec_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/backend/backend.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, ir)\u001b[0m\n\u001b[1;32m     91\u001b[0m         return ir.typ._from_json(\n\u001b[1;32m     92\u001b[0m             Env.hail().backend.spark.SparkBackend.executeJSON(\n\u001b[0;32m---> 93\u001b[0;31m                 self._to_java_ir(ir)))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = tb.rename({'f0': 'locus', 'f1':'rsid','f2':'pos','f5':'maf','f7':'info'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = tb.f0.split(':')\n",
    "tb = tb.annotate(locus = hl.locus(loc[0],hl.int(loc[1].split('_')[0]), reference_genome='GRCh37'))\n",
    "tb = tb.key_by('locus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mt = mt.key_rows_by().key_rows_by('locus')\n",
    "# tb = tb.annotate(foo = mt.rows()[tb.locus].rsid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb1 = tb.filter(hl.is_defined(mt.rows()[tb.locus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-11 13:57:07 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:09 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:11 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:12 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:13 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:15 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:16 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:17 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:19 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:20 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:21 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:23 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:24 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:27 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:27 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:29 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:30 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:31 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:32 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-11 13:57:32 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "HailException: array index out of bounds: 1 / 1. IR: (ArrayRef\n  (Apply split\n    (GetField f0\n      (In Struct{f0:String,f1:String,f2:Int32,f3:String,f4 ...\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1227 in stage 240.0 failed 20 times, most recent failure: Lost task 1227.19 in stage 240.0 (TID 10441, ukbb-nb-w-4.c.ukbb-round2.internal, executor 22): is.hail.utils.HailException: array index out of bounds: 1 / 1. IR: (ArrayRef\n  (Apply split\n    (GetField f0\n      (In Struct{f0:String,f1:String,f2:Int32,f3:String,f4 ...\n\tat is.hail.codegen.generated.C135.method1(Unknown Source)\n\tat is.hail.codegen.generated.C135.apply(Unknown Source)\n\tat is.hail.codegen.generated.C135.apply(Unknown Source)\n\tat is.hail.expr.ir.TableMapRows$$anonfun$41$$anonfun$apply$12.apply(TableIR.scala:921)\n\tat is.hail.expr.ir.TableMapRows$$anonfun$41$$anonfun$apply$12.apply(TableIR.scala:920)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:46)\n\tat is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:38)\n\tat is.hail.utils.package$.using(package.scala:594)\n\tat is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:38)\n\tat is.hail.rvd.RVD$$anonfun$35.apply(RVD.scala:1056)\n\tat is.hail.rvd.RVD$$anonfun$35.apply(RVD.scala:1054)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:944)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:944)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:943)\n\tat is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:196)\n\tat is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1060)\n\tat is.hail.rvd.RVD$.makeCoercer(RVD.scala:1124)\n\tat is.hail.rvd.RVD$.coerce(RVD.scala:1082)\n\tat is.hail.rvd.RVD.changeKey(RVD.scala:105)\n\tat is.hail.rvd.RVD.changeKey(RVD.scala:102)\n\tat is.hail.rvd.RVDType$$anonfun$enforceKey$4.apply(RVDType.scala:156)\n\tat is.hail.rvd.RVDType$$anonfun$enforceKey$4.apply(RVDType.scala:156)\n\tat is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:259)\n\tat is.hail.expr.ir.TableLeftJoinRightDistinct.execute(TableIR.scala:773)\n\tat is.hail.expr.ir.TableFilter.execute(TableIR.scala:333)\n\tat is.hail.expr.ir.TableMapRows.execute(TableIR.scala:800)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:754)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:87)\n\tat is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:31)\n\tat is.hail.backend.spark.SparkBackend$.execute(SparkBackend.scala:49)\n\tat is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:16)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\nis.hail.utils.HailException: array index out of bounds: 1 / 1. IR: (ArrayRef\n  (Apply split\n    (GetField f0\n      (In Struct{f0:String,f1:String,f2:Int32,f3:String,f4 ...\n\tat is.hail.codegen.generated.C135.method1(Unknown Source)\n\tat is.hail.codegen.generated.C135.apply(Unknown Source)\n\tat is.hail.codegen.generated.C135.apply(Unknown Source)\n\tat is.hail.expr.ir.TableMapRows$$anonfun$41$$anonfun$apply$12.apply(TableIR.scala:921)\n\tat is.hail.expr.ir.TableMapRows$$anonfun$41$$anonfun$apply$12.apply(TableIR.scala:920)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:46)\n\tat is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:38)\n\tat is.hail.utils.package$.using(package.scala:594)\n\tat is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:38)\n\tat is.hail.rvd.RVD$$anonfun$35.apply(RVD.scala:1056)\n\tat is.hail.rvd.RVD$$anonfun$35.apply(RVD.scala:1054)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:944)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\nHail version: 0.2.12-13681278eb89\nError summary: HailException: array index out of bounds: 1 / 1. IR: (ArrayRef\n  (Apply split\n    (GetField f0\n      (In Struct{f0:String,f1:String,f2:Int32,f3:String,f4 ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8d9e76be5d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://nbaya/risk_gradients/info_scores.tsv.bgz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-926>\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, output, types_file, header, parallel, delimiter)\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/table.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, output, types_file, header, parallel, delimiter)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         Env.backend().execute(\n\u001b[1;32m   1007\u001b[0m             TableWrite(self._tir, TableTextWriter(output, types_file, header,\n\u001b[0;32m-> 1008\u001b[0;31m                                                   Env.hail().utils.ExportType.getExportType(parallel), delimiter)))\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnamed_exprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'GroupedTable'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/backend/backend.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, ir)\u001b[0m\n\u001b[1;32m     91\u001b[0m         return ir.typ._from_json(\n\u001b[1;32m     92\u001b[0m             Env.hail().backend.spark.SparkBackend.executeJSON(\n\u001b[0;32m---> 93\u001b[0;31m                 self._to_java_ir(ir)))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hail/hail.zip/hail/utils/java.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m    227\u001b[0m                              \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                              'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: HailException: array index out of bounds: 1 / 1. IR: (ArrayRef\n  (Apply split\n    (GetField f0\n      (In Struct{f0:String,f1:String,f2:Int32,f3:String,f4 ...\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1227 in stage 240.0 failed 20 times, most recent failure: Lost task 1227.19 in stage 240.0 (TID 10441, ukbb-nb-w-4.c.ukbb-round2.internal, executor 22): is.hail.utils.HailException: array index out of bounds: 1 / 1. IR: (ArrayRef\n  (Apply split\n    (GetField f0\n      (In Struct{f0:String,f1:String,f2:Int32,f3:String,f4 ...\n\tat is.hail.codegen.generated.C135.method1(Unknown Source)\n\tat is.hail.codegen.generated.C135.apply(Unknown Source)\n\tat is.hail.codegen.generated.C135.apply(Unknown Source)\n\tat is.hail.expr.ir.TableMapRows$$anonfun$41$$anonfun$apply$12.apply(TableIR.scala:921)\n\tat is.hail.expr.ir.TableMapRows$$anonfun$41$$anonfun$apply$12.apply(TableIR.scala:920)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:46)\n\tat is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:38)\n\tat is.hail.utils.package$.using(package.scala:594)\n\tat is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:38)\n\tat is.hail.rvd.RVD$$anonfun$35.apply(RVD.scala:1056)\n\tat is.hail.rvd.RVD$$anonfun$35.apply(RVD.scala:1054)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:944)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1575)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1563)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1562)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1562)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1790)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1745)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1734)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:944)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:943)\n\tat is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:196)\n\tat is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1060)\n\tat is.hail.rvd.RVD$.makeCoercer(RVD.scala:1124)\n\tat is.hail.rvd.RVD$.coerce(RVD.scala:1082)\n\tat is.hail.rvd.RVD.changeKey(RVD.scala:105)\n\tat is.hail.rvd.RVD.changeKey(RVD.scala:102)\n\tat is.hail.rvd.RVDType$$anonfun$enforceKey$4.apply(RVDType.scala:156)\n\tat is.hail.rvd.RVDType$$anonfun$enforceKey$4.apply(RVDType.scala:156)\n\tat is.hail.expr.ir.TableKeyBy.execute(TableIR.scala:259)\n\tat is.hail.expr.ir.TableLeftJoinRightDistinct.execute(TableIR.scala:773)\n\tat is.hail.expr.ir.TableFilter.execute(TableIR.scala:333)\n\tat is.hail.expr.ir.TableMapRows.execute(TableIR.scala:800)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:754)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:87)\n\tat is.hail.expr.ir.CompileAndEvaluate$.apply(CompileAndEvaluate.scala:31)\n\tat is.hail.backend.spark.SparkBackend$.execute(SparkBackend.scala:49)\n\tat is.hail.backend.spark.SparkBackend$.executeJSON(SparkBackend.scala:16)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\nis.hail.utils.HailException: array index out of bounds: 1 / 1. IR: (ArrayRef\n  (Apply split\n    (GetField f0\n      (In Struct{f0:String,f1:String,f2:Int32,f3:String,f4 ...\n\tat is.hail.codegen.generated.C135.method1(Unknown Source)\n\tat is.hail.codegen.generated.C135.apply(Unknown Source)\n\tat is.hail.codegen.generated.C135.apply(Unknown Source)\n\tat is.hail.expr.ir.TableMapRows$$anonfun$41$$anonfun$apply$12.apply(TableIR.scala:921)\n\tat is.hail.expr.ir.TableMapRows$$anonfun$41$$anonfun$apply$12.apply(TableIR.scala:920)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.next(Iterator.scala:444)\n\tat is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:46)\n\tat is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:38)\n\tat is.hail.utils.package$.using(package.scala:594)\n\tat is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:38)\n\tat is.hail.rvd.RVD$$anonfun$35.apply(RVD.scala:1056)\n\tat is.hail.rvd.RVD$$anonfun$35.apply(RVD.scala:1054)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$32.apply(ContextRDD.scala:422)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:944)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:344)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\nHail version: 0.2.12-13681278eb89\nError summary: HailException: array index out of bounds: 1 / 1. IR: (ArrayRef\n  (Apply split\n    (GetField f0\n      (In Struct{f0:String,f1:String,f2:Int32,f3:String,f4 ..."
     ]
    }
   ],
   "source": [
    "tb1.export('gs://nbaya/risk_gradients/info_scores.tsv.bgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 21:05:53 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:06:27 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:07 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:12 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:13 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:14 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:15 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:16 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:18 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:19 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:20 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:21 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:21 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:22 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:23 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:23 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:24 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:27 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:27 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-06-10 21:07:28 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "mt.rows().select('info').write('gs://nbaya/risk_gradients/info_scores.ht')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}