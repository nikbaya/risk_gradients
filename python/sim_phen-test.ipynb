{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import requests\n",
    "import datetime\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import argparse\n",
    "#from hail.experimental.ldscsim import simulate_phenotypes\n",
    "url = 'https://raw.githubusercontent.com/nikbaya/ldscsim/master/ldscsim.py'\n",
    "r = requests.get(url).text\n",
    "exec(r)\n",
    "calculate_phenotypes = calculate_phenotypes\n",
    "simulate_phenotypes = simulate_phenotypes\n",
    "\n",
    "\n",
    "wd = 'gs://nbaya/risk_gradients/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.4\n",
      "SparkUI available at http://ukbb-nb-m.c.ukbb-round2.internal:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.25-04344d214361\n",
      "LOGGING: writing to /tmp/foo.log\n"
     ]
    }
   ],
   "source": [
    "hl.init(log='/tmp/foo.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_pt(variant_set, maf, h2, pi, use_1kg_eur_hm3_snps, chrom, is_pruned,\n",
    "                threshold=None, max_reps=60):\n",
    "    # read in testing mt\n",
    "    mt = hl.read_matrix_table(\n",
    "        wd+f'genotypes{(\"\" if chrom is \"all\" else f\".chr{chrom}\")}.all.{variant_set}.maf_{maf}{\".1kg_eur_hm3\" if use_1kg_eur_hm3_snps else \"\"}.mt')\n",
    "    n_train = int(300e3)\n",
    "    seed = 1\n",
    "    train = hl.import_table(\n",
    "        wd+f'iid.sim.train.n_{n_train}.seed_{seed}.tsv.bgz').key_by('s')\n",
    "    test = mt.filter_cols(hl.is_defined(train[mt.s]), keep=False)\n",
    "    sim_cols = hl.read_table(\n",
    "        wd+f'sim.cols{(\"\" if chrom is \"all\" else f\".chr{chrom}\")}.all.{variant_set}.maf_{maf}.h2_{h2}.pi_{pi}{\".1kg_eur_hm3\" if use_1kg_eur_hm3_snps else \"\"}.ht')\n",
    "    test = test.annotate_cols(sim_y=sim_cols[test.s].y)\n",
    "    ct_cols = test.count_cols()\n",
    "    print(f'\\n###############\\ntest mt col count: {ct_cols}\\n###############')\n",
    "\n",
    "    if is_pruned:\n",
    "\n",
    "        # define the set of SNPs\n",
    "        pruned_snps_file = 'ukb_imp_v3_pruned.bim' #from Robert Maier (pruning threshold=0.2)\n",
    "        variants = hl.import_table(\n",
    "            wd+pruned_snps_file, delimiter='\\t', no_header=True, impute=True)\n",
    "        print(f'\\n... Pruning to variants in {wd+pruned_snps_file}...')\n",
    "        variants = variants.rename(\n",
    "            {'f0': 'chr', 'f1': 'rsid', 'f3': 'pos'}).key_by('rsid')\n",
    "        test = test.key_rows_by('rsid')\n",
    "        # filter to variants defined in variants table\n",
    "        test = test.filter_rows(hl.is_defined(variants[test.rsid]))\n",
    "        ct_rows = test.count_rows()\n",
    "        print(f'\\n###############\\ntest mt row count after pruning filter: {ct_rows}\\n###############')\n",
    "    else:\n",
    "        print(f'\\n... Not pruning because is_pruned={is_pruned} ...')\n",
    "\n",
    "#     if threshold!=None:\n",
    "# #         ss = hl.import_table(wd+f'sim.gwas.h2_obs_0.765.pi_obs_0.001.n_train_sub_100000.subset_0.1kg_eur_hm3.v2.tsv.gz',\n",
    "# #                                  impute=True,\n",
    "# #                                  force_bgz=True)\n",
    "#         ss = hl.read_table('gs://nbaya/risk_gradients/sim.rows.chr22.all.qc_pos.maf_0.05.h2_0.75.pi_0.01.1kg_eur_hm3.ht/')\n",
    "#         ss = ss.rename({'rsid':'SNP','A1_1kg':'A1','beta':'BETA'})\n",
    "#         ss = ss.annotate(P = 0)\n",
    "#         ss = ss.key_by('SNP')\n",
    "#         test = test.filter_rows(hl.is_defined(ss[test.rsid]))\n",
    "#         ct_rows = test.count_rows()\n",
    "#         print(f'\\n###############\\ntest mt row count after checking against GWAS for 100k: {ct_rows}\\n###############')\n",
    "    \n",
    "#         threshold = 1\n",
    "    \n",
    "#         print(\n",
    "#             f'\\n###############\\npval threshold: pval<{threshold}\\n###############')\n",
    "#         test = test.annotate_rows(P=ss[test.rsid].P)\n",
    "#         test = test.filter_rows(test.P < threshold)\n",
    "#         ct_rows = test.count_rows()\n",
    "#         print(\n",
    "#             f'\\n###############\\ntest mt row count after keeping pval<{threshold}: {ct_rows}\\n###############')\n",
    "\n",
    "    n_train_subs = [100e3, 50e3, 20e3, 10e3, 5e3]  # 20e3,\n",
    "    n_train_subs = [int(x) for x in n_train_subs]\n",
    "    n_train_sub_ls = []\n",
    "    subset_ls = []\n",
    "    r_ls = []  # list of correlation coefficients\n",
    "    \n",
    "    n_train = 300e3\n",
    "    sim_beta = hl.read_table('gs://nbaya/risk_gradients/sim.rows.chr22.all.qc_pos.maf_0.05.h2_0.75.pi_0.01.1kg_eur_hm3.ht/')\n",
    "    sim_beta = sim_beta.rename({'rsid':'SNP','beta':'true_beta'})\n",
    "    sim_beta = sim_beta.key_by('SNP')\n",
    "    print(sim_beta.describe())\n",
    "\n",
    "    for n_train_sub in n_train_subs:\n",
    "        # number of training subsets in full training set with n_train_sub individuals each\n",
    "        n_subsets = int(n_train/n_train_sub)\n",
    "        for subset in range(n_subsets)[:max_reps]:\n",
    "#            ss_path = wd + \\\n",
    "#                f'sim.gwas.h2_obs_0.765.pi_obs_0.001.n_train_sub_{n_train_sub}.subset_{subset}{\".1kg_eur_hm3\" if use_1kg_eur_hm3_snps else \"\"}.v2.tsv.gz'\n",
    "#            ss_path = wd+f'sim.chr22.gwas.h2_obs_0.485.pi_obs_0.0009.n_train_sub_{n_train_sub}.subset_{subset}.1kg_eur_hm3.v2.tsv.gz'\n",
    "            ss_path = wd+f'sim.chr22.gwas.h2_obs_0.7.pi_obs_0.0084.n_train_sub_{n_train_sub}.subset_{subset}.1kg_eur_hm3.v2.tsv.gz'\n",
    "#             ss_path = wd+f'/sim.chr22.gwas.h2_obs_0.746.pi_obs_1.0.n_train_sub_{n_train_sub}.subset_{subset}.1kg_eur_hm3.v2.tsv.gz'\n",
    "#             print(\n",
    "#                 f'\\n#############\\nn_train_sub={n_train_sub} (subset {subset+1} of {n_subsets})\\nss path: {ss_path}\\n#############')\n",
    "            try:\n",
    "                ss = hl.import_table(ss_path,impute=True,force=True,key='SNP')\n",
    "            except:\n",
    "                print(f'file does not exist: {ss_path}')\n",
    "                next\n",
    "            ss = ss.rename({'BETA':'gwas_beta'})\n",
    "            ss = ss.annotate(BETA = (sim_beta[ss.SNP].true_beta!=0)*ss.gwas_beta)\n",
    "            \n",
    "            test1 = test.annotate_rows(adj_beta=ss[test.rsid].BETA,\n",
    "                                       prs_A1=ss[test.rsid].A1)\n",
    "            test1 = test1.annotate_rows(adj_beta_flipped=(test1.adj_beta*(test1.alleles[0] == test1.prs_A1) +\n",
    "                                                          -test1.adj_beta*(test1.alleles[0] != test1.prs_A1)))  # flip sign if A1 is not the same in both\n",
    "            test1 = calculate_phenotypes(\n",
    "                mt=test1, genotype=test1.dosage, beta=test1.adj_beta_flipped, h2=1)\n",
    "            test1 = test1.rename({'y': 'prs'})\n",
    "            r = test1.aggregate_cols(hl.agg.corr(test1.sim_y, test1.prs))\n",
    "            n_train_sub_ls.append(n_train_sub)\n",
    "            subset_ls.append(subset)\n",
    "            r_ls.append(r)\n",
    "            print(\n",
    "                f'\\n#############\\nr for subset {subset+1} of {n_subsets} (n_train_sub={n_train_sub}): {r}\\nr2 for subset {subset+1} of {n_subsets}: {r**2}\\n#############')\n",
    "    df = pd.DataFrame(data=list(zip(n_train_sub_ls, subset_ls, [ct_cols]*len(n_train_sub_ls), r_ls)),\n",
    "                          columns=['n_train', 'subset_id', 'n_test', 'r'])\n",
    "    print(df)\n",
    "\n",
    "    hl.Table.from_pandas(df).export(wd+f'prs_cs/corr.pt{\".is_pruned\" if is_pruned else \"\"}.threshold_{threshold}.{variant_set}.maf_{maf}.h2_{h2}.pi_{pi}{\".1kg_eur_hm3\" if use_1kg_eur_hm3_snps else \"\"}_truly_causal_marginal_betas.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_set = 'qc_pos'\n",
    "maf=0.05\n",
    "h2=0.75\n",
    "pi=0.01\n",
    "use_1kg_eur_hm3_snps=True\n",
    "is_pruned=False\n",
    "chrom=22\n",
    "max_reps=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:15:14 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 's' as type 'str' (type not specified)\n",
      "  Loading column 'label_100000' as type 'str' (type not specified)\n",
      "  Loading column 'label_50000' as type 'str' (type not specified)\n",
      "  Loading column 'label_20000' as type 'str' (type not specified)\n",
      "  Loading column 'label_10000' as type 'str' (type not specified)\n",
      "  Loading column 'label_5000' as type 'str' (type not specified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###############\n",
      "test mt col count: 61144\n",
      "###############\n",
      "\n",
      "... Not pruning because is_pruned=False ...\n",
      "----------------------------------------\n",
      "Global fields:\n",
      "    'ldscsim': struct {\n",
      "        h2: float64, \n",
      "        pi: array<float64>\n",
      "    } \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37> \n",
      "    'alleles': array<str> \n",
      "    'SNP': str \n",
      "    'varid': str \n",
      "    'A1_1kg': str \n",
      "    'A2_1kg': str \n",
      "    'AF': float64 \n",
      "    'true_beta': float64 \n",
      "----------------------------------------\n",
      "Key: ['SNP']\n",
      "----------------------------------------\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:15:26 Hail: INFO: Reading table to impute column types\n",
      "2019-12-05 14:15:26 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-05 14:15:47 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:15:47 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:15:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:16:00 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:16:03 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:16:03 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:16:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:16:11 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "r for subset 1 of 3 (n_train_sub=100000): 0.7751967188739658\n",
      "r2 for subset 1 of 3: 0.6009299529529624\n",
      "#############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:17:23 Hail: INFO: Reading table to impute column types\n",
      "2019-12-05 14:17:23 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-05 14:17:47 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:17:47 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:17:48 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:17:55 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:17:56 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:17:56 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:17:57 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:18:03 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "r for subset 2 of 3 (n_train_sub=100000): 0.7744895590513459\n",
      "r2 for subset 2 of 3: 0.5998340770795482\n",
      "#############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:19:02 Hail: INFO: Reading table to impute column types\n",
      "2019-12-05 14:19:02 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-05 14:19:14 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:19:14 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:19:15 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:19:22 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:19:22 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:19:23 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:19:24 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:19:29 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "r for subset 3 of 3 (n_train_sub=100000): 0.7746323997303085\n",
      "r2 for subset 3 of 3: 0.6000553547119365\n",
      "#############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:20:50 Hail: INFO: Reading table to impute column types\n",
      "2019-12-05 14:20:50 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-05 14:21:09 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:21:09 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:21:10 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:21:17 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:21:18 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:21:19 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:21:19 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:21:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "r for subset 1 of 6 (n_train_sub=50000): 0.7739574489422755\n",
      "r2 for subset 1 of 6: 0.5990101327732349\n",
      "#############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:22:25 Hail: INFO: Reading table to impute column types\n",
      "2019-12-05 14:22:25 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-05 14:22:44 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:22:44 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:22:45 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:22:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:22:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:22:52 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:22:53 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:22:59 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "r for subset 2 of 6 (n_train_sub=50000): 0.7760782506379965\n",
      "r2 for subset 2 of 6: 0.6022974511133329\n",
      "#############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:24:12 Hail: INFO: Reading table to impute column types\n",
      "2019-12-05 14:24:13 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-05 14:24:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:24:25 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:24:26 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:24:32 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:24:33 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:24:33 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:24:34 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:24:40 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "r for subset 3 of 6 (n_train_sub=50000): 0.7774077200117145\n",
      "r2 for subset 3 of 6: 0.6043627631338123\n",
      "#############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:25:56 Hail: INFO: Reading table to impute column types\n",
      "2019-12-05 14:25:56 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-05 14:26:21 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:26:21 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:26:22 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:26:28 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:26:29 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:26:29 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:26:30 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:26:36 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "r for subset 4 of 6 (n_train_sub=50000): 0.7722259571563674\n",
      "r2 for subset 4 of 6: 0.5963329289060678\n",
      "#############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:27:40 Hail: INFO: Reading table to impute column types\n",
      "2019-12-05 14:27:40 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-05 14:27:58 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:27:58 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:27:59 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:28:05 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:28:06 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:28:06 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-05 14:28:07 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-05 14:28:13 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############\n",
      "r for subset 5 of 6 (n_train_sub=50000): 0.7710868853835959\n",
      "r2 for subset 5 of 6: 0.5945749848105748\n",
      "#############\n",
      "file does not exist: gs://nbaya/risk_gradients/sim.chr22.gwas.h2_obs_0.7.pi_obs_0.0084.n_train_sub_50000.subset_5.1kg_eur_hm3.v2.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-05 14:29:25 Hail: WARN: 'gs://nbaya/risk_gradients/sim.chr22.gwas.h2_obs_0.7.pi_obs_0.0084.n_train_sub_50000.subset_5.1kg_eur_hm3.v2.tsv.gz' refers to no files\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot rename 'BETA' to 'gwas_beta': field already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-51364592d7ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m get_corr_pt(variant_set=variant_set, maf=maf, h2=h2, pi=pi,\n\u001b[1;32m      2\u001b[0m             \u001b[0muse_1kg_eur_hm3_snps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_1kg_eur_hm3_snps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_pruned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_pruned\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             threshold=1,chrom=chrom,max_reps=max_reps)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-1de82fd8c99b>\u001b[0m in \u001b[0;36mget_corr_pt\u001b[0;34m(variant_set, maf, h2, pi, use_1kg_eur_hm3_snps, chrom, is_pruned, threshold, max_reps)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'file does not exist: {ss_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'BETA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'gwas_beta'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBETA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msim_beta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSNP\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_beta\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgwas_beta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/conda/default/lib/python3.6/site-packages/decorator.py:decorator-gen-1053>\u001b[0m in \u001b[0;36mrename\u001b[0;34m(self, mapping)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/default/lib/python3.6/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/default/lib/python3.6/site-packages/hail/table.py\u001b[0m in \u001b[0;36mrename\u001b[0;34m(self, mapping)\u001b[0m\n\u001b[1;32m   2432\u001b[0m                         repr(seen[v]), repr(k), repr(v)))\n\u001b[1;32m   2433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2434\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot rename {} to {}: field already exists.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2435\u001b[0m             \u001b[0mseen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_row_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot rename 'BETA' to 'gwas_beta': field already exists."
     ]
    }
   ],
   "source": [
    "get_corr_pt(variant_set=variant_set, maf=maf, h2=h2, pi=pi,\n",
    "            use_1kg_eur_hm3_snps=use_1kg_eur_hm3_snps,is_pruned=is_pruned,\n",
    "            threshold=1,chrom=chrom,max_reps=max_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-04 13:03:27 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 's' as type 'str' (type not specified)\n",
      "  Loading column 'label_100000' as type 'str' (type not specified)\n",
      "  Loading column 'label_50000' as type 'str' (type not specified)\n",
      "  Loading column 'label_20000' as type 'str' (type not specified)\n",
      "  Loading column 'label_10000' as type 'str' (type not specified)\n",
      "  Loading column 'label_5000' as type 'str' (type not specified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in testing mt\n",
    "mt = hl.read_matrix_table(\n",
    "    wd+f'genotypes{(\"\" if chrom is \"all\" else f\".chr{chrom}\")}.all.{variant_set}.maf_{maf}{\".1kg_eur_hm3\" if use_1kg_eur_hm3_snps else \"\"}.mt')\n",
    "n_train = int(300e3)\n",
    "seed = 1\n",
    "train = hl.import_table(\n",
    "    wd+f'iid.sim.train.n_{n_train}.seed_{seed}.tsv.bgz').key_by('s')\n",
    "test = mt.filter_cols(hl.is_defined(train[mt.s]), keep=False)\n",
    "sim_cols = hl.read_table(\n",
    "    wd+f'sim.cols{(\"\" if chrom is \"all\" else f\".chr{chrom}\")}.all.{variant_set}.maf_{maf}.h2_{h2}.pi_{pi}{\".1kg_eur_hm3\" if use_1kg_eur_hm3_snps else \"\"}.ht')\n",
    "test = test.annotate_cols(sim_y=sim_cols[test.s].y)\n",
    "# ct_cols = test.count_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'ldscsim': struct {\n",
      "        h2: float64, \n",
      "        pi: array<float64>\n",
      "    } \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37> \n",
      "    'alleles': array<str> \n",
      "    'SNP': str \n",
      "    'varid': str \n",
      "    'A1_1kg': str \n",
      "    'A2_1kg': str \n",
      "    'AF': float64 \n",
      "    'true_beta': float64 \n",
      "    'A1': str \n",
      "----------------------------------------\n",
      "Key: ['SNP']\n",
      "----------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "n_train_subs = [100e3, 50e3, 20e3, 10e3, 5e3]  # 20e3,\n",
    "n_train_subs = [int(x) for x in n_train_subs]\n",
    "n_train_sub_ls = []\n",
    "subset_ls = []\n",
    "r_ls = []  # list of correlation coefficients\n",
    "\n",
    "n_train = 300e3\n",
    "sim_beta = hl.read_table('gs://nbaya/risk_gradients/sim.rows.chr22.all.qc_pos.maf_0.05.h2_0.75.pi_0.01.1kg_eur_hm3.ht/')\n",
    "sim_beta = sim_beta.annotate(A1 = sim_beta.alleles[0])\n",
    "sim_beta = sim_beta.rename({'rsid':'SNP','beta':'true_beta'})\n",
    "sim_beta = sim_beta.key_by('SNP')\n",
    "print(sim_beta.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-04 14:08:34 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:08:36 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:08:46 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:08:47 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:08:48 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:08:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "n_train_sub=5000\n",
    "# number of training subsets in full training set with n_train_sub individuals each\n",
    "subset=0\n",
    "# ss_path = wd+f'sim.chr22.gwas.h2_obs_0.7.pi_obs_0.0084.n_train_sub_{n_train_sub}.subset_{subset}.1kg_eur_hm3.v2.tsv.gz'\n",
    "# ss = hl.import_table(ss_path,impute=True,force=True,key='SNP')\n",
    "# ss = ss.rename({'BETA':'gwas_beta'})\n",
    "ss=sim_beta.rename({'true_beta':'BETA'})\n",
    "test1 = test.annotate_rows(adj_beta=ss[test.rsid].BETA,\n",
    "                           prs_A1=ss[test.rsid].A1)\n",
    "test1 = test1.annotate_rows(adj_beta_flipped=(test1.adj_beta*(test1.alleles[0] == test1.prs_A1) +\n",
    "                                              -test1.adj_beta*(test1.alleles[0] != test1.prs_A1)))  # flip sign if A1 is not the same in both\n",
    "test1 = calculate_phenotypes(mt=test1, genotype=test1.dosage, beta=test1.adj_beta_flipped, h2=1)\n",
    "test1 = test1.rename({'y': 'prs'})\n",
    "r = test1.aggregate_cols(hl.agg.corr(test1.sim_y, test1.prs))\n",
    "n_train_sub_ls.append(n_train_sub)\n",
    "subset_ls.append(subset)\n",
    "r_ls.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8363386698641746"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6994623707101768"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test1.annotate_cols(y_no_noise = sim_cols[test1.s].y_no_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-04 14:12:59 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:13:01 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:13:05 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:13:08 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:13:09 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:13:13 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "r = test1.aggregate_cols(hl.agg.corr(test1.prs, test1.y_no_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6995533238122902"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_cols.aggregate(hl.agg.stats(sim_cols.y_no_noise)).stdev**2/sim_cols.aggregate(hl.agg.stats(sim_cols.y)).stdev**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999895054076228"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-04 14:10:46 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:10:47 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:10:50 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:10:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:10:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 14:10:55 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td>prs_A1</td><td>adj_beta</td><td>adj_beta_flipped</td></tr>\n",
       "<tr><td>locus&lt;GRCh37&gt;</td><td>array&lt;str&gt;</td><td>str</td><td>float64</td><td>float64</td></tr>\n",
       "</thead><tbody><tr><td>22:18080154</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>&quot;A&quot;</td><td>9.24e-02</td><td>9.24e-02</td></tr>\n",
       "<tr><td>22:18319179</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>&quot;C&quot;</td><td>1.67e-02</td><td>1.67e-02</td></tr>\n",
       "<tr><td>22:19030113</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>&quot;A&quot;</td><td>2.03e-02</td><td>2.03e-02</td></tr>\n",
       "<tr><td>22:19197949</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>&quot;G&quot;</td><td>-6.10e-02</td><td>-6.10e-02</td></tr>\n",
       "<tr><td>22:19232466</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>&quot;G&quot;</td><td>9.60e-02</td><td>9.60e-02</td></tr>\n",
       "<tr><td>22:19506456</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>&quot;G&quot;</td><td>1.95e-02</td><td>1.95e-02</td></tr>\n",
       "<tr><td>22:19665133</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>&quot;G&quot;</td><td>-5.93e-02</td><td>-5.93e-02</td></tr>\n",
       "<tr><td>22:19690250</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>&quot;A&quot;</td><td>-7.59e-02</td><td>-7.59e-02</td></tr>\n",
       "<tr><td>22:19921378</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>&quot;C&quot;</td><td>5.89e-02</td><td>5.89e-02</td></tr>\n",
       "<tr><td>22:20012286</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>&quot;A&quot;</td><td>-4.30e-03</td><td>-4.30e-03</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+------------+--------+-----------+------------------+\n",
       "| locus         | alleles    | prs_A1 |  adj_beta | adj_beta_flipped |\n",
       "+---------------+------------+--------+-----------+------------------+\n",
       "| locus<GRCh37> | array<str> | str    |   float64 |          float64 |\n",
       "+---------------+------------+--------+-----------+------------------+\n",
       "| 22:18080154   | [\"A\",\"G\"]  | \"A\"    |  9.24e-02 |         9.24e-02 |\n",
       "| 22:18319179   | [\"C\",\"T\"]  | \"C\"    |  1.67e-02 |         1.67e-02 |\n",
       "| 22:19030113   | [\"A\",\"G\"]  | \"A\"    |  2.03e-02 |         2.03e-02 |\n",
       "| 22:19197949   | [\"G\",\"A\"]  | \"G\"    | -6.10e-02 |        -6.10e-02 |\n",
       "| 22:19232466   | [\"G\",\"A\"]  | \"G\"    |  9.60e-02 |         9.60e-02 |\n",
       "| 22:19506456   | [\"G\",\"T\"]  | \"G\"    |  1.95e-02 |         1.95e-02 |\n",
       "| 22:19665133   | [\"G\",\"A\"]  | \"G\"    | -5.93e-02 |        -5.93e-02 |\n",
       "| 22:19690250   | [\"A\",\"G\"]  | \"A\"    | -7.59e-02 |        -7.59e-02 |\n",
       "| 22:19921378   | [\"C\",\"T\"]  | \"C\"    |  5.89e-02 |         5.89e-02 |\n",
       "| 22:20012286   | [\"A\",\"G\"]  | \"A\"    | -4.30e-03 |        -4.30e-03 |\n",
       "+---------------+------------+--------+-----------+------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test1.filter_rows(test1.adj_beta!=0).select_rows('prs_A1','adj_beta','adj_beta_flipped').rows().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_alt = sim_cols.aggregate(hl.agg.corr(sim_cols.y, sim_cols.y_no_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8366737392792422"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000229459995093"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_alt**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>s</td><td>y</td><td>y_no_noise</td></tr>\n",
       "<tr><td>str</td><td>float64</td><td>float64</td></tr>\n",
       "</thead><tbody><tr><td>&quot;1000019&quot;</td><td>3.97e-01</td><td>2.99e-01</td></tr>\n",
       "<tr><td>&quot;1000022&quot;</td><td>-1.12e+00</td><td>-1.01e+00</td></tr>\n",
       "<tr><td>&quot;1000035&quot;</td><td>-1.01e+00</td><td>-5.21e-01</td></tr>\n",
       "<tr><td>&quot;1000046&quot;</td><td>1.47e+00</td><td>1.03e+00</td></tr>\n",
       "<tr><td>&quot;1000054&quot;</td><td>-9.07e-01</td><td>-3.13e-02</td></tr>\n",
       "<tr><td>&quot;1000063&quot;</td><td>1.65e+00</td><td>9.57e-01</td></tr>\n",
       "<tr><td>&quot;1000081&quot;</td><td>9.53e-01</td><td>4.89e-01</td></tr>\n",
       "<tr><td>&quot;1000090&quot;</td><td>-1.32e+00</td><td>-1.17e+00</td></tr>\n",
       "<tr><td>&quot;1000105&quot;</td><td>3.51e-01</td><td>5.73e-02</td></tr>\n",
       "<tr><td>&quot;1000129&quot;</td><td>2.29e-01</td><td>3.29e-01</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+-----------+-----------+------------+\n",
       "| s         |         y | y_no_noise |\n",
       "+-----------+-----------+------------+\n",
       "| str       |   float64 |    float64 |\n",
       "+-----------+-----------+------------+\n",
       "| \"1000019\" |  3.97e-01 |   2.99e-01 |\n",
       "| \"1000022\" | -1.12e+00 |  -1.01e+00 |\n",
       "| \"1000035\" | -1.01e+00 |  -5.21e-01 |\n",
       "| \"1000046\" |  1.47e+00 |   1.03e+00 |\n",
       "| \"1000054\" | -9.07e-01 |  -3.13e-02 |\n",
       "| \"1000063\" |  1.65e+00 |   9.57e-01 |\n",
       "| \"1000081\" |  9.53e-01 |   4.89e-01 |\n",
       "| \"1000090\" | -1.32e+00 |  -1.17e+00 |\n",
       "| \"1000105\" |  3.51e-01 |   5.73e-02 |\n",
       "| \"1000129\" |  2.29e-01 |   3.29e-01 |\n",
       "+-----------+-----------+------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_cols.select('y','y_no_noise').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "    'isFemale': bool\n",
      "    'age': int32\n",
      "    'age_squared': int32\n",
      "    'age_isFemale': int32\n",
      "    'age_squared_isFemale': int32\n",
      "    'PC1': float64\n",
      "    'PC2': float64\n",
      "    'PC3': float64\n",
      "    'PC4': float64\n",
      "    'PC5': float64\n",
      "    'PC6': float64\n",
      "    'PC7': float64\n",
      "    'PC8': float64\n",
      "    'PC9': float64\n",
      "    'PC10': float64\n",
      "    'PC11': float64\n",
      "    'PC12': float64\n",
      "    'PC13': float64\n",
      "    'PC14': float64\n",
      "    'PC15': float64\n",
      "    'PC16': float64\n",
      "    'PC17': float64\n",
      "    'PC18': float64\n",
      "    'PC19': float64\n",
      "    'PC20': float64\n",
      "    'sim_y': float64\n",
      "    'y_no_noise': float64\n",
      "    'prs': float64\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'varid': str\n",
      "    'A1_1kg': str\n",
      "    'A2_1kg': str\n",
      "    'AF': float64\n",
      "    'adj_beta': float64\n",
      "    'prs_A1': str\n",
      "    'adj_beta_flipped': float64\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'dosage': float64\n",
      "    'norm_gt': float64\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-04 13:36:51 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 13:36:53 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 13:36:59 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 13:37:01 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 13:37:02 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-04 13:37:06 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>s</td><td>y_no_noise</td><td>prs</td></tr>\n",
       "<tr><td>str</td><td>float64</td><td>float64</td></tr>\n",
       "</thead><tbody><tr><td>&quot;1000022&quot;</td><td>7.76e-01</td><td>7.76e-01</td></tr>\n",
       "<tr><td>&quot;1000046&quot;</td><td>-9.36e-01</td><td>-9.36e-01</td></tr>\n",
       "<tr><td>&quot;1000141&quot;</td><td>1.14e-01</td><td>1.14e-01</td></tr>\n",
       "<tr><td>&quot;1000198&quot;</td><td>-1.04e+00</td><td>-1.04e+00</td></tr>\n",
       "<tr><td>&quot;1000236&quot;</td><td>-3.51e-01</td><td>-3.51e-01</td></tr>\n",
       "<tr><td>&quot;1000255&quot;</td><td>-1.74e+00</td><td>-1.74e+00</td></tr>\n",
       "<tr><td>&quot;1000291&quot;</td><td>4.77e-01</td><td>4.77e-01</td></tr>\n",
       "<tr><td>&quot;1000396&quot;</td><td>-1.22e+00</td><td>-1.22e+00</td></tr>\n",
       "<tr><td>&quot;1000408&quot;</td><td>-1.54e+00</td><td>-1.54e+00</td></tr>\n",
       "<tr><td>&quot;1000415&quot;</td><td>-1.42e-01</td><td>-1.42e-01</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+-----------+------------+-----------+\n",
       "| s         | y_no_noise |       prs |\n",
       "+-----------+------------+-----------+\n",
       "| str       |    float64 |   float64 |\n",
       "+-----------+------------+-----------+\n",
       "| \"1000022\" |   7.76e-01 |  7.76e-01 |\n",
       "| \"1000046\" |  -9.36e-01 | -9.36e-01 |\n",
       "| \"1000141\" |   1.14e-01 |  1.14e-01 |\n",
       "| \"1000198\" |  -1.04e+00 | -1.04e+00 |\n",
       "| \"1000236\" |  -3.51e-01 | -3.51e-01 |\n",
       "| \"1000255\" |  -1.74e+00 | -1.74e+00 |\n",
       "| \"1000291\" |   4.77e-01 |  4.77e-01 |\n",
       "| \"1000396\" |  -1.22e+00 | -1.22e+00 |\n",
       "| \"1000408\" |  -1.54e+00 | -1.54e+00 |\n",
       "| \"1000415\" |  -1.42e-01 | -1.42e-01 |\n",
       "+-----------+------------+-----------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test1.cols().select('y_no_noise','prs').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-03 22:19:18 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 's' as type 'str' (type not specified)\n",
      "  Loading column 'label_100000' as type 'str' (type not specified)\n",
      "  Loading column 'label_50000' as type 'str' (type not specified)\n",
      "  Loading column 'label_20000' as type 'str' (type not specified)\n",
      "  Loading column 'label_10000' as type 'str' (type not specified)\n",
      "  Loading column 'label_5000' as type 'str' (type not specified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###############\n",
      "test mt col count: 61144\n",
      "###############\n",
      "\n",
      "... Not pruning because is_pruned=False ...\n",
      "----------------------------------------\n",
      "Global fields:\n",
      "    'ldscsim': struct {\n",
      "        h2: float64, \n",
      "        pi: array<float64>\n",
      "    } \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37> \n",
      "    'alleles': array<str> \n",
      "    'SNP': str \n",
      "    'varid': str \n",
      "    'A1': str \n",
      "    'A2_1kg': str \n",
      "    'AF': float64 \n",
      "    'true_beta': float64 \n",
      "----------------------------------------\n",
      "Key: ['SNP']\n",
      "----------------------------------------\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-03 22:19:30 Hail: INFO: Reading table to impute column types\n",
      "2019-12-03 22:19:30 Hail: INFO: Finished type imputation\n",
      "  Loading column 'SNP' as type 'str' (imputed)\n",
      "  Loading column 'A1' as type 'str' (imputed)\n",
      "  Loading column 'A2' as type 'str' (imputed)\n",
      "  Loading column 'BETA' as type 'float64' (imputed)\n",
      "  Loading column 'P' as type 'float64' (imputed)\n",
      "2019-12-03 22:19:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-03 22:19:52 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-03 22:19:56 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-03 22:20:08 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-03 22:20:10 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-03 22:20:10 Hail: INFO: Coerced sorted dataset\n"
     ]
    }
   ],
   "source": [
    "get_corr_pt(variant_set=variant_set, maf=maf, h2=h2, pi=pi,\n",
    "            use_1kg_eur_hm3_snps=use_1kg_eur_hm3_snps,is_pruned=is_pruned,\n",
    "            threshold=1,chrom=chrom,max_reps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>locus</td><td>alleles</td><td>beta</td></tr>\n",
       "<tr><td>locus&lt;GRCh37&gt;</td><td>array&lt;str&gt;</td><td>float64</td></tr>\n",
       "</thead><tbody><tr><td>22:16886873</td><td>[&quot;G&quot;,&quot;A&quot;]</td><td>-0.00e+00</td></tr>\n",
       "<tr><td>22:16892858</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>0.00e+00</td></tr>\n",
       "<tr><td>22:17054720</td><td>[&quot;T&quot;,&quot;C&quot;]</td><td>0.00e+00</td></tr>\n",
       "<tr><td>22:17056415</td><td>[&quot;T&quot;,&quot;C&quot;]</td><td>0.00e+00</td></tr>\n",
       "<tr><td>22:17057138</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>-0.00e+00</td></tr>\n",
       "<tr><td>22:17058616</td><td>[&quot;C&quot;,&quot;T&quot;]</td><td>0.00e+00</td></tr>\n",
       "<tr><td>22:17067504</td><td>[&quot;G&quot;,&quot;T&quot;]</td><td>-0.00e+00</td></tr>\n",
       "<tr><td>22:17072483</td><td>[&quot;A&quot;,&quot;G&quot;]</td><td>-0.00e+00</td></tr>\n",
       "<tr><td>22:17074622</td><td>[&quot;A&quot;,&quot;C&quot;]</td><td>0.00e+00</td></tr>\n",
       "<tr><td>22:17075353</td><td>[&quot;C&quot;,&quot;A&quot;]</td><td>-0.00e+00</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+---------------+------------+-----------+\n",
       "| locus         | alleles    |      beta |\n",
       "+---------------+------------+-----------+\n",
       "| locus<GRCh37> | array<str> |   float64 |\n",
       "+---------------+------------+-----------+\n",
       "| 22:16886873   | [\"G\",\"A\"]  | -0.00e+00 |\n",
       "| 22:16892858   | [\"A\",\"G\"]  |  0.00e+00 |\n",
       "| 22:17054720   | [\"T\",\"C\"]  |  0.00e+00 |\n",
       "| 22:17056415   | [\"T\",\"C\"]  |  0.00e+00 |\n",
       "| 22:17057138   | [\"G\",\"T\"]  | -0.00e+00 |\n",
       "| 22:17058616   | [\"C\",\"T\"]  |  0.00e+00 |\n",
       "| 22:17067504   | [\"G\",\"T\"]  | -0.00e+00 |\n",
       "| 22:17072483   | [\"A\",\"G\"]  | -0.00e+00 |\n",
       "| 22:17074622   | [\"A\",\"C\"]  |  0.00e+00 |\n",
       "| 22:17075353   | [\"C\",\"A\"]  | -0.00e+00 |\n",
       "+---------------+------------+-----------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tb.beta.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}